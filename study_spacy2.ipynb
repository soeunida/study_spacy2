{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 언어 모델 다운로드\r\n",
    "!python -m spacy download en_core_web_md"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "doc = nlp(\"I went there\")\r\n",
    "\r\n",
    "for token in doc:\r\n",
    "    print(token, type(token), token.text, type(token.text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I <class 'spacy.tokens.token.Token'> I <class 'str'>\n",
      "went <class 'spacy.tokens.token.Token'> went <class 'str'>\n",
      "there <class 'spacy.tokens.token.Token'> there <class 'str'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "doc = nlp(\"I own a pretty cat.\")\r\n",
    "\r\n",
    "print ([ token.text for token in doc ], type([ token.text for token in doc ]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['I', 'own', 'a', 'pretty', 'cat', '.'] <class 'list'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "doc = nlp(\"It's been a crazy week!!!\")\r\n",
    "\r\n",
    "print ([ token.text for token in doc ], type([ token.text for token in doc ]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['It', \"'s\", 'been', 'a', 'crazy', 'week', '!', '!', '!'] <class 'list'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# custom tokenizer\r\n",
    "import spacy\r\n",
    "from spacy.symbols import ORTH # orthography를 의미 (맞춤법)\r\n",
    "\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "doc = nlp(\"lemme that\")\r\n",
    "print ([ token.text for token in doc ], type([ token.text for token in doc ]))\r\n",
    "\r\n",
    "special_case1 = [ {ORTH: \"lem\"}, {ORTH: \"me\"} ]\r\n",
    "special_case2 = [ {ORTH: \"Lem\"}, {ORTH: \"me\"} ]\r\n",
    "nlp.tokenizer.add_special_case(\"lemme\", special_case1)\r\n",
    "nlp.tokenizer.add_special_case(\"Lemme\", special_case2)\r\n",
    "doc = nlp(\"lemme that!!!\")\r\n",
    "print ([ token.text for token in doc ], type([ token.text for token in doc ]))\r\n",
    "\r\n",
    "doc = nlp(\"Let's try again! Lemme that!, lemme\")\r\n",
    "print ([ token.text for token in doc ], type([ token.text for token in doc ]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['lemme', 'that'] <class 'list'>\n",
      "['lem', 'me', 'that', '!', '!', '!'] <class 'list'>\n",
      "['Let', \"'s\", 'try', 'again', '!', 'Lem', 'me', 'that', '!', ',', 'lem', 'me'] <class 'list'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# custom tokenizer - 문장기호도 custom tokenizer에 포함될 수 있는 경우\r\n",
    "import spacy\r\n",
    "from spacy.symbols import ORTH # orthography를 의미 (맞춤법)\r\n",
    "\r\n",
    "special_case = [ {ORTH: \"...lemme...?\"} ]\r\n",
    "nlp.tokenizer.add_special_case(\"...lemme...?\", special_case)\r\n",
    "doc = nlp(\"I have a dream. ...lemme...?\")\r\n",
    "print ([ token.text for token in doc ], type([ token.text for token in doc ]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['I', 'have', 'a', 'dream', '.', '...lemme...?'] <class 'list'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# tokenizer의 debugging\r\n",
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "text = \"Let's go! Lemme\"\r\n",
    "doc = nlp(text)\r\n",
    "print ([ token.text for token in doc ])\r\n",
    "\r\n",
    "detail_tokens = nlp.tokenizer.explain(text) \r\n",
    "for detail_token in detail_tokens:\r\n",
    "    print(detail_token[1], \"\\t\", detail_token[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Let', \"'s\", 'go', '!', 'Lemme']\n",
      "Let \t SPECIAL-1\n",
      "'s \t SPECIAL-2\n",
      "go \t TOKEN\n",
      "! \t SUFFIX\n",
      "Lemme \t TOKEN\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# Sentence segmentation은 tokenization보다 좀더 복잡한 작업\r\n",
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "text = \"부산 해운대해수욕장에서 중학생 3명이 물놀이를 하던 중 1명이 실종되고 1명이 숨지는 사고가 발생했다. 25일 경찰과 소방당국에 따르면 이날 오전 3시 41분께 부산 해운대해수욕장에서 중학생 3명이 물놀이 하던 중 실종됐다는 신고가 접수됐다.\"\r\n",
    "doc = nlp(text)\r\n",
    "for sentence in doc.sents:\r\n",
    "    print(sentence)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "부산 해운대해수욕장에서 중학생 3명이 물놀이를 하던 중 1명이 실종되고 1명이 숨지는 사고가 발생했다.\n",
      "25일 경찰과 소방당국에 따르면 이날 오전 3시 41분께 부산 해운대해수욕장에서 중학생 3명이 물놀이 하던 중\n",
      "실종됐다는 신고가 접수됐다.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# lemma : token의 기본 형태 (base form), 사전에서 token의 기본형으로 찾을 수 있다.\r\n",
    "# eating의 lemma => eat / eats의 lemma => eat / ate의 lemma => eat\r\n",
    "# lemmatization : token을 자신의 lemma로 찾아가는 과정 \r\n",
    "\r\n",
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "text = \"I went there for working and worked for 3 years.\"\r\n",
    "doc = nlp(text)\r\n",
    "for token in doc:\r\n",
    "    print(token.text, \"\\t\", token.lemma_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I \t I\n",
      "went \t go\n",
      "there \t there\n",
      "for \t for\n",
      "working \t working\n",
      "and \t and\n",
      "worked \t work\n",
      "for \t for\n",
      "3 \t 3\n",
      "years \t year\n",
      ". \t .\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "import spacy\r\n",
    "from spacy.symbols import ORTH, LEMMA\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "special_case = [ {ORTH: \"Angeltown\", LEMMA: \"Los Angeles\"} ]\r\n",
    "nlp.tokenizer.add_special_case(\"Angeltown\", special_case)\r\n",
    "\r\n",
    "doc = nlp(\"I am flying to Angeltown\")\r\n",
    "for token in doc:\r\n",
    "    print(token.text, token.lemma_)\r\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "[E1005] Unable to set attribute 'LEMMA' in tokenizer exception for 'Angeltown'. Tokenizer exceptions are only allowed to specify ORTH and NORM.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp/ipykernel_12764/4199565432.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mspecial_case\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mORTH\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Angeltown\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLEMMA\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Los Angeles\"\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_special_case\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Angeltown\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecial_case\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I am flying to Angeltown\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\20210725\\venv_20210725\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\20210725\\venv_20210725\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._validate_special_case\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E1005] Unable to set attribute 'LEMMA' in tokenizer exception for 'Angeltown'. Tokenizer exceptions are only allowed to specify ORTH and NORM."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "doc = nlp(\"I know that you have been to Korea.\")\r\n",
    "for token in doc:\r\n",
    "    print(token)\r\n",
    "\r\n",
    "print(doc[2:4])\r\n",
    "print(doc[4:])\r\n",
    "print(doc[3:-1])\r\n",
    "print(doc[6:])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I\n",
      "know\n",
      "that\n",
      "you\n",
      "have\n",
      "been\n",
      "to\n",
      "Korea\n",
      ".\n",
      "that you\n",
      "have been to Korea.\n",
      "you have been to Korea\n",
      "to Korea.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "doc = nlp(\"I know that you have been to Korea.\")\r\n",
    "span = doc[2:4]\r\n",
    "for token in span:\r\n",
    "    print(token)\r\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "that\n",
      "you\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "doc = nlp(\"Hello, hi!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "doc[0].lower_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc = nlp(\"HELLO, Hello, hello, hEllo\")\r\n",
    "for token in doc:\r\n",
    "    print(token.text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(doc[0].is_upper)\r\n",
    "print(doc[0].is_lower)\r\n",
    "print(doc[1].is_upper)\r\n",
    "print(doc[1].is_lower)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "doc = nlp(\"Cat and Cat123\")\r\n",
    "print(doc[0].is_alpha)\r\n",
    "print(doc[1].is_alpha)\r\n",
    "print(doc[2].is_alpha) # nonalphabetic에는 숫자, 기호, 공백 문자를 포함"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "doc = nlp(\"English and 한글!\")\r\n",
    "print(doc[0].is_ascii)\r\n",
    "print(doc[2].is_ascii)\r\n",
    "print(doc[3].is_ascii)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "doc = nlp(\"Cat Cat123 123\")\r\n",
    "print(doc[0].is_digit)\r\n",
    "print(doc[1].is_digit)\r\n",
    "print(doc[2].is_digit)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "doc = nlp(\"Hey, You and me!\")\r\n",
    "print(doc[1].is_punct)\r\n",
    "print(doc[4].is_punct)\r\n",
    "print(doc[5].is_punct)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('venv_20210725': venv)"
  },
  "interpreter": {
   "hash": "b2c46294d02003512898a675212ce284c66238a44ec8c080f36cf0db005bb14d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}